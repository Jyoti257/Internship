{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066cf2b7-69e4-4f9d-8727-c938205df12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6fa7c-0869-4c3e-bab8-4bd4c7723dd9",
   "metadata": {},
   "source": [
    " Q-1:In this question you have to scrape data using the filters available on the webpage You have to use the location and \n",
    "salary filter.  \n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.  \n",
    "You have to scrape the job-title, job-location, company name, experience required.  \n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs  \n",
    "The task will be done as shown in the below steps:  \n",
    "1. first get the web page https://www.naukri.com/ \n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.  \n",
    "3. Then click the search button.  \n",
    "4. Then apply the location filter and salary filter by checking the respective boxes  \n",
    "5. Then scrape the data for the first 10 jobs results you get.  \n",
    "6. Finally create a dataframe of the scraped data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af244394-5b7e-4314-917b-347809295559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Function to initialize webdriver and get the webpage\n",
    "def get_webpage(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "# Function to search for \"Data Scientist\" and click search button\n",
    "def search_jobs(driver):\n",
    "    search_box = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"qsb-keyword-sugg\"))\n",
    "    )\n",
    "    search_box.send_keys(\"Data Scientist\")\n",
    "    search_button = driver.find_element_by_class_name(\"search-btn\")\n",
    "    search_button.click()\n",
    "\n",
    "# Function to apply location and salary filters\n",
    "def apply_filters(driver):\n",
    "    location_filter = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//label[@for='chk-Delhi / NCR-cityTypeGid-']/i\"))\n",
    "    )\n",
    "    location_filter.click()\n",
    "    salary_filter = driver.find_element_by_xpath(\"//label[@for='chk-3-6 Lakhs-annual_ctc']/i\")\n",
    "    salary_filter.click()\n",
    "\n",
    "# Function to scrape data for first 10 job results\n",
    "def scrape_jobs(driver):\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    job_cards = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".jobTuple.bgWhite.br4.mb-8\"))\n",
    "    )\n",
    "\n",
    "    for job_card in job_cards[:10]:\n",
    "        job_title = job_card.find_element_by_css_selector(\".title.fw500.ellipsis\").text\n",
    "        job_titles.append(job_title)\n",
    "        job_location = job_card.find_element_by_css_selector(\".fleft.grey-text.br2.placeHolderLi.location\").text\n",
    "        job_locations.append(job_location)\n",
    "        company_name = job_card.find_element_by_css_selector(\".fleft.grey-text.br2.placeHolderLi.company\").text\n",
    "        company_names.append(company_name)\n",
    "        experience = job_card.find_element_by_css_selector(\".fleft.grey-text.br2.placeHolderLi.experience\").text\n",
    "        experience_required.append(experience)\n",
    "\n",
    "    return job_titles, job_locations, company_names, experience_required\n",
    "\n",
    "# Main function to orchestrate the scraping process\n",
    "def main():\n",
    "    url = \"https://www.naukri.com/\"\n",
    "    driver = get_webpage(url)\n",
    "    search_jobs(driver)\n",
    "    apply_filters(driver)\n",
    "    job_titles, job_locations, company_names, experience_required = scrape_jobs(driver)\n",
    "\n",
    "    # Create a DataFrame to store the scraped data\n",
    "    jobs_df = pd.DataFrame({\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(jobs_df)\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34efceb2-c8f9-4a62-827b-a2a25d44b315",
   "metadata": {},
   "source": [
    " Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the \n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.shine.com/ \n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field. \n",
    "3. Then click the searchbutton.  \n",
    "4. Then scrape the data for the first 10 jobs results you get.  \n",
    "5. Finally create a dataframe of the scraped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1491bb-0cc2-4fd9-98c9-208a1de1c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Function to initialize webdriver and get the webpage\n",
    "def get_webpage(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "# Function to search for \"Data Scientist\" jobs in \"Bangalore\" location and click search button\n",
    "def search_jobs(driver):\n",
    "    job_title_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"q\"))\n",
    "    )\n",
    "    job_title_input.send_keys(\"Data Scientist\")\n",
    "\n",
    "    location_input = driver.find_element_by_id(\"l\")\n",
    "    location_input.clear()\n",
    "    location_input.send_keys(\"Bangalore\")\n",
    "\n",
    "    search_button = driver.find_element_by_id(\"jb\")\n",
    "    search_button.click()\n",
    "\n",
    "# Function to scrape data for first 10 job results\n",
    "def scrape_jobs(driver):\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "\n",
    "    job_cards = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".search_listing.srl_card\"))\n",
    "    )\n",
    "\n",
    "    for job_card in job_cards[:10]:\n",
    "        job_title = job_card.find_element_by_css_selector(\".srl_heading\").text\n",
    "        job_titles.append(job_title)\n",
    "        job_location = job_card.find_element_by_css_selector(\".srl_location\").text\n",
    "        job_locations.append(job_location)\n",
    "        company_name = job_card.find_element_by_css_selector(\".srl_cmp_name\").text\n",
    "        company_names.append(company_name)\n",
    "        experience = job_card.find_element_by_css_selector(\".srl_exp\").text\n",
    "        experience_required.append(experience)\n",
    "\n",
    "    return job_titles, job_locations, company_names, experience_required\n",
    "\n",
    "# Main function to orchestrate the scraping process\n",
    "def main():\n",
    "    url = \"https://www.shine.com/\"\n",
    "    driver = get_webpage(url)\n",
    "    search_jobs(driver)\n",
    "    job_titles, job_locations, company_names, experience_required = scrape_jobs(driver)\n",
    "\n",
    "    # Create a DataFrame to store the scraped data\n",
    "    jobs_df = pd.DataFrame({\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(jobs_df)\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498db1dd-2c2d-4501-856d-e95f397d0317",
   "metadata": {},
   "source": [
    " Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\n",
    "reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=F\n",
    " LIPKART \n",
    "As shown in the above page you have to scrape the tick marked attributes. These are: \n",
    "1. Rating \n",
    "2. Review summary \n",
    "3. Full review \n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dce18-8a4a-41d4-b34e-5847d4991574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape reviews data using Selenium\n",
    "def scrape_reviews():\n",
    "    # Initialize the Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the Flipkart reviews page for iPhone 11\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "    \n",
    "    # Loop to scrape data from multiple pages until 100 reviews are scraped\n",
    "    reviews_count = 0\n",
    "    while reviews_count < 100:\n",
    "        # Wait for the reviews container to be visible\n",
    "        reviews_container = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div._27M-vq\"))\n",
    "        )\n",
    "        \n",
    "        # Scroll to load all reviews\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", reviews_container)\n",
    "        \n",
    "        # Find all the review cards on the page\n",
    "        review_cards = reviews_container.find_elements(By.XPATH, \"//div[contains(@class, '_1AtVbE')]\")\n",
    "        \n",
    "        # Extract data from each review card\n",
    "        for card in review_cards:\n",
    "            rating = card.find_element(By.CSS_SELECTOR, \"div._3LWZlK._1BLPMq\").text\n",
    "            review_summary = card.find_element(By.CSS_SELECTOR, \"p._2-N8zT\").text\n",
    "            full_review = card.find_element(By.CSS_SELECTOR, \"div.qwjRop\").text\n",
    "            \n",
    "            # Append data to lists\n",
    "            ratings.append(rating)\n",
    "            review_summaries.append(review_summary)\n",
    "            full_reviews.append(full_review)\n",
    "            \n",
    "            reviews_count += 1\n",
    "            if reviews_count == 100:\n",
    "                break\n",
    "        \n",
    "        # If 100 reviews are not yet scraped, click on the next button to load more reviews\n",
    "        if reviews_count < 100:\n",
    "            next_button = driver.find_element(By.XPATH, \"//span[contains(text(), 'Next')]\")\n",
    "            next_button.click()\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return ratings, review_summaries, full_reviews\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    ratings, review_summaries, full_reviews = scrape_reviews()\n",
    "    \n",
    "    # Create a DataFrame to store the scraped data\n",
    "    reviews_df = pd.DataFrame({\n",
    "        \"Rating\": ratings,\n",
    "        \"Review Summary\": review_summaries,\n",
    "        \"Full Review\": full_reviews\n",
    "    })\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(reviews_df)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c767b2-464d-47cc-9f75-067a2191fe17",
   "metadata": {},
   "source": [
    " Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search \n",
    "field. \n",
    "You have to scrape 3 attributes of each sneaker: \n",
    "1. Brand \n",
    "2. Product Description \n",
    "3. Price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2637f19-eaab-464a-b567-e4fff769aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for sneakers using Selenium\n",
    "def scrape_sneakers():\n",
    "    # Initialize the Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Open the Flipkart website\n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "    \n",
    "    # Wait for the search bar to be visible\n",
    "    search_bar = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.NAME, \"q\"))\n",
    "    )\n",
    "    \n",
    "    # Enter \"sneakers\" in the search bar and press Enter\n",
    "    search_bar.send_keys(\"sneakers\" + Keys.RETURN)\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    brands = []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "    \n",
    "    # Loop to scrape data from multiple pages until 100 sneakers are scraped\n",
    "    sneakers_count = 0\n",
    "    while sneakers_count < 100:\n",
    "        # Wait for the page to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"_1AtVbE\"))\n",
    "        )\n",
    "        \n",
    "        # Find all the sneaker cards on the page\n",
    "        sneaker_cards = driver.find_elements(By.CLASS_NAME, \"_1fQZEK\")\n",
    "        \n",
    "        # Extract data from each sneaker card\n",
    "        for card in sneaker_cards:\n",
    "            brand = card.find_element(By.CLASS_NAME, \"_2WkVRV\").text\n",
    "            description = card.find_element(By.CLASS_NAME, \"IRpwTa\").text\n",
    "            price = card.find_element(By.CLASS_NAME, \"_30jeq3\").text\n",
    "            \n",
    "            # Append data to lists\n",
    "            brands.append(brand)\n",
    "            descriptions.append(description)\n",
    "            prices.append(price)\n",
    "            \n",
    "            sneakers_count += 1\n",
    "            if sneakers_count == 100:\n",
    "                break\n",
    "        \n",
    "        # If 100 sneakers are not yet scraped, click on the next button to load more sneakers\n",
    "        if sneakers_count < 100:\n",
    "            next_button = driver.find_element(By.CLASS_NAME, \"_1LKTO3\")\n",
    "            next_button.click()\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    return brands, descriptions, prices\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    brands, descriptions, prices = scrape_sneakers()\n",
    "    \n",
    "    # Create a DataFrame to store the scraped data\n",
    "    sneakers_df = pd.DataFrame({\n",
    "        \"Brand\": brands,\n",
    "        \"Product Description\": descriptions,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(sneakers_df)\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e5c61-9b77-42eb-86da-319a01e6c663",
   "metadata": {},
   "source": [
    "Q5: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU \n",
    "Type filter to “Intel Core i7” as shown in the below image: \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop: \n",
    "1. Title \n",
    "2. Ratings \n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afbc39a-85ff-407d-b7dc-964e98d59c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Acer Aspire Lite 12th Gen Intel Core i7-1255U Premium Metal Laptop (Windows 11 Home/16 GB RAM/512 GB SSD) AL15-52, 39.62cm (15.6\") Full HD Display, Metal Body, Steel Gray, 1.59 Kg\n",
      "Rating: 3.8 out of 5 stars\n",
      "Price: 54,990\n",
      "\n",
      "\n",
      "Title: Dell Inspiron 7420 2in1 Laptop, Intel Core i7-1255U Processor/16GB/512GB/14.0\" (35.56cm) FHD+ WVA Touch 250 nits, Active Pen/Win 11 + MSO'21, 15 Month McAfee/Backlit KB + FPR/Platinum Silver\n",
      "Rating: 4.0 out of 5 stars\n",
      "Price: 77,490\n",
      "\n",
      "\n",
      "Title: Lenovo IdeaPad Slim 3 Intel Core i7 12th Gen 15.6 inch (39.62cm) FHD Thin & Light Laptop (16GB/512GB SSD/Windows 11/Office 2021/3months Game Pass/Arctic Grey/1.63Kg), 82RK011EIN\n",
      "Rating: 4.1 out of 5 stars\n",
      "Price: 62,120\n",
      "\n",
      "\n",
      "Title: MSI Modern 14, Intel 12th Gen. i7-1255U, 36CM FHD 60Hz Laptop (16GB/512GB NVMe SSD/Windows 11 Home/Intel Iris Xe Graphics/Classic Black/1.4Kg), C12M-459IN\n",
      "Rating: 4.0 out of 5 stars\n",
      "Price: 54,990\n",
      "\n",
      "\n",
      "Title: ASUS Vivobook 15, IntelCore i7-12650H 12th Gen, 15.6\" (39.62 cm) FHD, Thin and Light Laptop (16 GB RAM/512GB SSD/Win11/Office 2021/Backlit/42WHr /Silver/1.75), X1502ZA-EJ742WS\n",
      "Rating: 3.7 out of 5 stars\n",
      "Price: 57,990\n",
      "\n",
      "\n",
      "Title: Acer Travelmate Business Laptop Intel Core i7-11th Gen (Windows 11 Home/16 GB Ram/1TB SSD/Intel Iris Xe Graphics/14.0 IPS Display/Backlit Keyboard/Fingerprint Sensor) TMP214-53\n",
      "Rating: 3.5 out of 5 stars\n",
      "Price: 47,990\n",
      "\n",
      "\n",
      "Title: HP Laptop 15s, 12th Gen Intel Core i7-1255U, 15.6-inch (39.6 cm), FHD, 16GB DDR4, 512GB SSD, Intel Iris Xe Graphics, Backlit KB, Thin & Light (Win 11, MSO 2021, Silver, 1.69 kg), fq5190TU\n",
      "Rating: 3.5 out of 5 stars\n",
      "Price: 63,990\n",
      "\n",
      "\n",
      "Title: Dell Latitude 7480 14in FHD Laptop PC - Intel Core i7-6600U 2.6GHz 16GB 512GB SSD Windows 10 Professional (Renewed)\n",
      "Rating: 4.0 out of 5 stars\n",
      "Price: 27,999\n",
      "\n",
      "\n",
      "Title: ASUS TUF Gaming F15, 15.6\"(39.62 cms) FHD 144Hz, Intel Core i7-11800H 11th Gen, 4GB NVIDIA GeForce RTX 3050 Ti, Gaming Laptop (16GB/512GB SSD/Windows 11/90WHrs Battery/Black/2.30 Kg), FX506HE-HN382W\n",
      "Rating: 4.4 out of 5 stars\n",
      "Price: 72,990\n",
      "\n",
      "\n",
      "Title: HP Pavilion 14 12th Gen Intel Core i7 16GB SDRAM/1TB SSD 14 inch(35.6cm) FHD,IPS,Micro-Edge Display/Intel Iris Xe Graphics/B&O/Win 11/Alexa Built-in/Backlit KB/FPR/MSO 2021/Natural Silver,14-dv2015TU\n",
      "Rating: 4.2 out of 5 stars\n",
      "Price: 77,700\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Open the browser and go to Amazon website\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver = webdriver.Chrome()  # Ensure you have Chromedriver installed\n",
    "driver.get(url)\n",
    "\n",
    "# Find the search field and enter \"Laptop\"\n",
    "search_field = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_field.send_keys(\"Laptop\")\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")))\n",
    "\n",
    "# Click on the filter for CPU Type and select \"Intel Core i7\"\n",
    "filter_button = driver.find_element(By.XPATH, \"//span[text()='CPU Type']/ancestor::div[@class='a-section a-spacing-none']\")\n",
    "filter_button.click()\n",
    "i7_filter = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\")\n",
    "i7_filter.click()\n",
    "\n",
    "# Wait for the filter to be applied\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")))\n",
    "\n",
    "# Scrape data for the first 10 laptops\n",
    "laptops = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")[:10]\n",
    "for laptop in laptops:\n",
    "    title = laptop.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "    try:\n",
    "        rating = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").get_attribute(\"innerHTML\")\n",
    "    except:\n",
    "        rating = \"No rating\"\n",
    "    try:\n",
    "        price = laptop.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "    except:\n",
    "        price = \"Price not available\"\n",
    "    print(\"Title:\", title)\n",
    "    print(\"Rating:\", rating)\n",
    "    print(\"Price:\", price)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e90933-b926-4765-9263-d041535ce289",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for Top 1000 Quotes of All Time. \n",
    "The above task will be done in following steps: \n",
    "1. First get the webpagehttps://www.azquotes.com/ \n",
    "2. Click on Top Quote \n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776d8e0-1fa2-4925-a44f-9acf0f2def64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to initialize webdriver and get the webpage\n",
    "def get_webpage(url):\n",
    "    # Initialize webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    # Get the webpage\n",
    "    driver.get(url)\n",
    "    return driver\n",
    "\n",
    "# Function to click on the \"Top Quote\" link\n",
    "def click_top_quote(driver):\n",
    "    # Find and click on the \"Top Quote\" link\n",
    "    top_quote_link = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'Top Quote')]\"))\n",
    "    )\n",
    "    top_quote_link.click()\n",
    "\n",
    "# Function to scrape the data for quotes\n",
    "def scrape_quotes(driver):\n",
    "    # Wait for the page to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.CLASS_NAME, \"quotes\")))\n",
    "    \n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parse the page source using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Find all quote elements\n",
    "    quotes = soup.find_all(\"div\", class_=\"wrap-block\")\n",
    "\n",
    "    # Initialize lists to store quote, author, and type of quote\n",
    "    quote_list = []\n",
    "    author_list = []\n",
    "    type_list = []\n",
    "\n",
    "    # Iterate through each quote element and extract data\n",
    "    for quote in quotes:\n",
    "        quote_text = quote.find(\"a\", class_=\"title\").text.strip()\n",
    "        author = quote.find(\"div\", class_=\"author\").text.strip()\n",
    "        quote_type = quote.find(\"div\", class_=\"kw-item\").text.strip()\n",
    "        \n",
    "        # Append data to respective lists\n",
    "        quote_list.append(quote_text)\n",
    "        author_list.append(author)\n",
    "        type_list.append(quote_type)\n",
    "\n",
    "    return quote_list, author_list, type_list\n",
    "\n",
    "# Main function to orchestrate the scraping process\n",
    "def main():\n",
    "    url = \"https://www.azquotes.com/\"\n",
    "    driver = get_webpage(url)\n",
    "    click_top_quote(driver)\n",
    "    quote_list, author_list, type_list = scrape_quotes(driver)\n",
    "\n",
    "    # Create a DataFrame to store the scraped data\n",
    "    quotes_df = pd.DataFrame({\n",
    "        'Quote': quote_list,\n",
    "        'Author': author_list,\n",
    "        'Type': type_list\n",
    "    })\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(quotes_df.head())\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cf52b-d6dd-44b2-b298-a02d7372b3e5",
   "metadata": {},
   "source": [
    "Q7: Write a python program to display list of respected former Prime Ministers of India (i.e. Name, \n",
    "Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of\n",
    "all-prime-ministers-of-india-1473165149-1  \n",
    "scrap the mentioned data and make the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5025d7f3-1115-4670-aad9-9f098f384e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   S.N.                      PM Name     Born-Dead  \\\n",
      "0    1.            Jawahar Lal Nehru   (1889–1964)   \n",
      "1    2.    Gulzarilal Nanda (Acting)   (1898-1998)   \n",
      "2    3.          Lal Bahadur Shastri   (1904–1966)   \n",
      "3   4.   Gulzari Lal Nanda  (Acting)   (1898-1998)   \n",
      "4    5.                Indira Gandhi   (1917–1984)   \n",
      "5    6.                Morarji Desai   (1896–1995)   \n",
      "6    7.                 Charan Singh   (1902–1987)   \n",
      "7    8.                Indira Gandhi   (1917–1984)   \n",
      "8    9.                 Rajiv Gandhi   (1944–1991)   \n",
      "9   10.                  V. P. Singh   (1931–2008)   \n",
      "10  11.              Chandra Shekhar   (1927–2007)   \n",
      "11  12.          P. V. Narasimha Rao   (1921–2004)   \n",
      "12  13.         Atal Bihari Vajpayee  (1924- 2018)   \n",
      "13  14.             H. D. Deve Gowda   (born 1933)   \n",
      "14  15.           Inder Kumar Gujral   (1919–2012)   \n",
      "15  16.         Atal Bihari Vajpayee   (1924-2018)   \n",
      "16  17.               Manmohan Singh   (born 1932)   \n",
      "17  18.                Narendra Modi   (born 1950)   \n",
      "18  19.                Narendra Modi   (born 1950)   \n",
      "\n",
      "                                       Term of office  \\\n",
      "0   15 August 1947 to 27 May 1964\\n16 years, 286 days   \n",
      "1                27 May 1964 to 9 June 1964,\\n13 days   \n",
      "2    9 June 1964 to 11 January 1966\\n1 year, 216 days   \n",
      "3         11 January 1966 to 24 January 1966\\n13 days   \n",
      "4   24 January 1966 to 24 March 1977\\n11 years, 59...   \n",
      "5   24 March 1977 to  28 July 1979 \\n2 year, 126 days   \n",
      "6           28 July 1979 to 14 January 1980\\n170 days   \n",
      "7   14 January 1980 to 31 October 1984\\n4 years, 2...   \n",
      "8   31 October 1984 to 2 December 1989\\n5 years, 3...   \n",
      "9       2 December 1989 to 10 November 1990\\n343 days   \n",
      "10         10 November 1990 to 21 June 1991\\n223 days   \n",
      "11     21 June 1991 to 16 May 1996\\n4 years, 330 days   \n",
      "12                16 May 1996 to 1 June 1996\\n16 days   \n",
      "13             1 June 1996 to 21 April 1997\\n324 days   \n",
      "14          21 April 1997 to 19 March 1998 \\n332 days   \n",
      "15    19 March 1998 to 22 May 2004 \\n6 years, 64 days   \n",
      "16    22 May 2004 to 26 May 2014   \\n10 years, 4 days   \n",
      "17                                 26 May 2014 - 2019   \n",
      "18                             30 May 2019- Incumbent   \n",
      "\n",
      "                                               Remark  \n",
      "0   The first prime minister of India and the long...  \n",
      "1                            First acting PM of India  \n",
      "2   He has given the slogan of 'Jai Jawan Jai Kisa...  \n",
      "3                                                   -  \n",
      "4                First female Prime Minister of India  \n",
      "5   Oldest to become PM (81 years old) and first t...  \n",
      "6             Only PM who did not face the Parliament  \n",
      "7   The first lady who served as PM for the second...  \n",
      "8                Youngest to become PM (40 years old)  \n",
      "9   First PM to step down after a vote of no confi...  \n",
      "10              He belongs to  Samajwadi Janata Party  \n",
      "11                          First PM from South India  \n",
      "12                             PM for shortest tenure  \n",
      "13                          He belongs to  Janata Dal  \n",
      "14                                             ------  \n",
      "15   The first non-congress PM who completed a ful...  \n",
      "16                                      First Sikh PM  \n",
      "17  4th Prime Minister of India who served two con...  \n",
      "18  First non-congress PM with two consecutive ten...  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (assuming ChromeDriver is in the PATH)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Give time for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Find the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//table')\n",
    "\n",
    "# Extract headers\n",
    "headers = table.find_elements(By.TAG_NAME, 'th')\n",
    "header_texts = [header.text for header in headers]\n",
    "\n",
    "# Extract rows\n",
    "rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "# Data extraction\n",
    "data = []\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_elements(By.TAG_NAME, 'td')\n",
    "    cols_text = [col.text for col in cols]\n",
    "    data.append(cols_text)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=header_texts)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a60f84-4999-430d-97cd-0a8aa4b8ea27",
   "metadata": {},
   "source": [
    "Q8: Write a python program to display list of 50 Most expensive cars in the world \n",
    "(i.e. Car name and Price) from https://www.motor1.com/  \n",
    "This task will be done in following steps: \n",
    "1. First get the webpage https://www.motor1.com/ \n",
    "2. Then You have to type in the search bar ’50 most expensive cars’ \n",
    "3. Then click on 50 most expensive cars in the world.. \n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44686ba2-d20b-4728-bd49-862f0cc4a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Initialize webdriver and get the webpage\n",
    "url = \"https://www.motor1.com/\"\n",
    "driver = webdriver.Chrome()  # Initialize webdriver\n",
    "driver.get(url)\n",
    "\n",
    "# Step 2: Find the search bar and type \"50 most expensive cars\"\n",
    "search_bar = driver.find_element_by_id('search-input')\n",
    "search_bar.clear()\n",
    "search_bar.send_keys(\"50 most expensive cars\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Step 3: Wait for page to load and click on the link for \"50 Most Expensive Cars In The World\"\n",
    "time.sleep(3)  # Wait for page to load\n",
    "link = driver.find_element_by_link_text(\"50 Most Expensive Cars In The World\")\n",
    "link.click()\n",
    "\n",
    "# Step 4: Scrape the data of the 50 most expensive cars\n",
    "time.sleep(3)  # Wait for page to load\n",
    "cars_data = []\n",
    "\n",
    "cars = driver.find_elements_by_class_name('item-price-title')\n",
    "for car in cars:\n",
    "    car_name = car.find_element_by_tag_name('h2').text.strip()\n",
    "    price = car.find_element_by_class_name('caption-text').text.strip()\n",
    "    cars_data.append({'Car Name': car_name, 'Price': price})\n",
    "\n",
    "# Step 5: Create a DataFrame\n",
    "df = pd.DataFrame(cars_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
